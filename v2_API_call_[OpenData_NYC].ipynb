{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYO4/I6Iu7jGYnLfJG34jk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WS2319/nyc_evictions_data/blob/main/v2_API_call_%5BOpenData_NYC%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehpoqkG9CP0V",
        "outputId": "b225b695-5af3-4552-aa1b-6ed50e69d2db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to fetch all eviction data...\n",
            "Fetching data with offset: 0...\n",
            "Fetching data with offset: 50000...\n",
            "Fetching data with offset: 100000...\n",
            "Fetched the last page of data.\n",
            "\n",
            "All data has been successfully fetched and combined!\n",
            "Total rows retrieved: 118386\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 118386 entries, 0 to 118385\n",
            "Data columns (total 20 columns):\n",
            " #   Column                      Non-Null Count   Dtype \n",
            "---  ------                      --------------   ----- \n",
            " 0   court_index_number          118386 non-null  object\n",
            " 1   docket_number               118386 non-null  object\n",
            " 2   eviction_address            118386 non-null  object\n",
            " 3   eviction_apt_num            100999 non-null  object\n",
            " 4   executed_date               118386 non-null  object\n",
            " 5   marshal_first_name          118386 non-null  object\n",
            " 6   marshal_last_name           118386 non-null  object\n",
            " 7   residential_commercial_ind  118386 non-null  object\n",
            " 8   borough                     118386 non-null  object\n",
            " 9   eviction_zip                118386 non-null  object\n",
            " 10  ejectment                   118386 non-null  object\n",
            " 11  eviction_possession         118386 non-null  object\n",
            " 12  latitude                    107901 non-null  object\n",
            " 13  longitude                   107901 non-null  object\n",
            " 14  community_board             107901 non-null  object\n",
            " 15  council_district            107901 non-null  object\n",
            " 16  census_tract                107901 non-null  object\n",
            " 17  bin                         107541 non-null  object\n",
            " 18  bbl                         107541 non-null  object\n",
            " 19  nta                         107901 non-null  object\n",
            "dtypes: object(20)\n",
            "memory usage: 18.1+ MB\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# Define the API endpoint\n",
        "api_url = \"https://data.cityofnewyork.us/resource/6z8x-wfk4.json\"\n",
        "\n",
        "# Define the page size (how many rows to get per request)\n",
        "page_size = 50000\n",
        "offset = 0\n",
        "\n",
        "# Create an empty list to store the DataFrames from each page\n",
        "all_data_dfs = []\n",
        "\n",
        "print(\"Starting to fetch all eviction data...\")\n",
        "\n",
        "while True:\n",
        "    # Set the parameters for the current page\n",
        "    params = {\n",
        "        \"$limit\": page_size,\n",
        "        \"$offset\": offset\n",
        "    }\n",
        "\n",
        "    print(f\"Fetching data with offset: {offset}...\")\n",
        "\n",
        "    try:\n",
        "        # Make the API request\n",
        "        response = requests.get(api_url, params=params)\n",
        "        response.raise_for_status()  # Raise an error for bad status codes\n",
        "\n",
        "        # Convert the JSON response to a DataFrame\n",
        "        data = response.json()\n",
        "\n",
        "        # If the response is empty, we've reached the end of the data\n",
        "        if not data:\n",
        "            print(\"No more data to fetch. Loop finished.\")\n",
        "            break\n",
        "\n",
        "        df_page = pd.DataFrame(data)\n",
        "\n",
        "        # Add the current page's DataFrame to our list\n",
        "        all_data_dfs.append(df_page)\n",
        "\n",
        "        # If we received fewer rows than we asked for, it must be the last page\n",
        "        if len(df_page) < page_size:\n",
        "            print(\"Fetched the last page of data.\")\n",
        "            break\n",
        "\n",
        "        # Increase the offset for the next iteration\n",
        "        offset += page_size\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        break\n",
        "\n",
        "# Concatenate all the page DataFrames into one final DataFrame\n",
        "if all_data_dfs:\n",
        "    final_df = pd.concat(all_data_dfs, ignore_index=True)\n",
        "    print(\"\\nAll data has been successfully fetched and combined!\")\n",
        "    print(f\"Total rows retrieved: {len(final_df)}\")\n",
        "    final_df.info()\n",
        "else:\n",
        "    print(\"\\nNo data was retrieved.\")\n",
        "\n",
        "# Now, 'final_df' contains ALL the eviction data from the API.\n",
        "# This is the DataFrame you will process and load into BigQuery."
      ]
    }
  ]
}